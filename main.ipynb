{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "stopword=nltk.corpus.stopwords.words('english')\n",
    "import string\n",
    "string.punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Importing the libraries that we are going to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We will save our trained model to the disk usi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In our case,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Here, we have bounded /api with the method pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We can again load the model by the following m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           body_text\n",
       "0  Importing the libraries that we are going to ...\n",
       "1  We will save our trained model to the disk usi...\n",
       "2                                      In our case, \n",
       "3  Here, we have bounded /api with the method pre...\n",
       "4  We can again load the model by the following m..."
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r=open(\"sadf.txt\").read() # a file that contains random text messages\n",
    "import pandas as pd\n",
    "d=pd.read_csv('sadf.txt', sep= '\\n',names=['body_text'],header=None)\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p={}\n",
    "type(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyser(r,d,p):\n",
    "   from nltk import tokenize\n",
    "   s=tokenize.sent_tokenize(r)\n",
    "   p['Sentences']=s  \n",
    "\n",
    "   import summa\n",
    "   from summa import summarizer\n",
    "   summary=(summarizer.summarize(r))\n",
    "   p['Summary']=summary #1\n",
    "    \n",
    "   noun=[]\n",
    "   import nltk\n",
    "   for sentence in s:\n",
    "    for word,pos in nltk.pos_tag(nltk.word_tokenize(str(sentence))):\n",
    "         if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "             noun.append(word) \n",
    "   p['Nouns']=noun[0:10] #2\n",
    "   verb=[]\n",
    "   for sentence in s:\n",
    "       for word,pos in nltk.pos_tag(nltk.word_tokenize(str(sentence))):\n",
    "           if (pos == 'VB' or pos == 'VDB' or pos == 'VBG' or pos == 'VBN' or pos =='VBP' or pos =='VBZ'):\n",
    "               verb.append(word)\n",
    "   p['Verbs']=verb[0:10]  #3\n",
    "\n",
    "   def rem_p(text):\n",
    "       text_nop=\"\".join([char for char in text if char not in string.punctuation])\n",
    "       return text_nop\n",
    "\n",
    "   d['body_text_clean']=d['body_text'].apply(lambda x: rem_p(x))\n",
    "\n",
    "   import re\n",
    "   def tokenize(text):\n",
    "       tokens=re.split('\\W+',text)\n",
    "       return tokens\n",
    "   d['body_text_tokenized']=d['body_text_clean'].apply(lambda x:tokenize(x.lower()))\n",
    "    \n",
    "   def remove_stopwords(tokenized_list):\n",
    "       text=[word for word in tokenized_list if word not in stopword]\n",
    "       return text\n",
    "   d['body_text_nostop']=d['body_text_tokenized'].apply(lambda x:remove_stopwords(x))    \n",
    "\n",
    "   ps=nltk.PorterStemmer()\n",
    "\n",
    "   def stemming(tokenized_text):\n",
    "       text=[ps.stem(word)for word in tokenized_text]\n",
    "       return text\n",
    "   d['body_text_stemmed']=d['body_text_nostop'].apply(lambda x:stemming(x))\n",
    "   st=d['body_text_stemmed'].tolist()\n",
    "   p['Stems']=st[0:1] \n",
    "\n",
    "   wn=nltk.WordNetLemmatizer()\n",
    "   def lemmatizing(tokenized_text):\n",
    "       text=[wn.lemmatize(word) for word in tokenized_text]\n",
    "       return text\n",
    "   d['body_text_lemmatized']=d['body_text_nostop'].apply(lambda x:lemmatizing(x))\n",
    "   l=d['body_text_lemmatized'].tolist()\n",
    "   p['Lemma']=l[0:1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sentences': ['\\x7fImporting the libraries that we are going to use to develop our model.',\n",
       "  'numpy and pandas to manipulate the matrices and data respectively, sklearn.model_selection for splitting data into train and test set and sklearn.linear_model to train our model using LinearRegression.',\n",
       "  'pickle to save our trained model to the disk, requests to send requests to the server and json to print the result in our terminal.',\n",
       "  'We will save our trained model to the disk using the pickle library.',\n",
       "  'Pickle is used to serializing and de-serializing a Python object structure.',\n",
       "  'In which python object is converted into the byte stream.',\n",
       "  'dump() method dumps the object into the file specified in the arguments.',\n",
       "  'In our case, \\nHere, we have bounded /api with the method predict().',\n",
       "  'In which predict method gets the data from the json passed by the requestor.',\n",
       "  'model.predict() method takes input from the json and converts it into 2D numpy array the results are stored into the variable named output and we return this variable after converting it into the json object using flasks jsonify() method.we want to save our model so that it can be used by the server.',\n",
       "  'So we will save our object regressor to the file named model.pkl.',\n",
       "  'We can again load the model by the following method,'],\n",
       " 'Summary': 'pickle to save our trained model to the disk, requests to send requests to the server and json to print the result in our terminal.\\nmodel.predict() method takes input from the json and converts it into 2D numpy array the results are stored into the variable named output and we return this variable after converting it into the json object using flasks jsonify() method.we want to save our model so that it can be used by the server.',\n",
       " 'Nouns': ['libraries',\n",
       "  'model',\n",
       "  'numpy',\n",
       "  'pandas',\n",
       "  'matrices',\n",
       "  'data',\n",
       "  'sklearn.model_selection',\n",
       "  'data',\n",
       "  'train',\n",
       "  'test'],\n",
       " 'Verbs': ['\\x7fImporting',\n",
       "  'are',\n",
       "  'going',\n",
       "  'use',\n",
       "  'develop',\n",
       "  'manipulate',\n",
       "  'splitting',\n",
       "  'train',\n",
       "  'using',\n",
       "  'save'],\n",
       " 'Stems': [['',\n",
       "   'import',\n",
       "   'librari',\n",
       "   'go',\n",
       "   'use',\n",
       "   'develop',\n",
       "   'model',\n",
       "   'numpi',\n",
       "   'panda',\n",
       "   'manipul',\n",
       "   'matric',\n",
       "   'data',\n",
       "   'respect',\n",
       "   'sklearnmodelselect',\n",
       "   'split',\n",
       "   'data',\n",
       "   'train',\n",
       "   'test',\n",
       "   'set',\n",
       "   'sklearnlinearmodel',\n",
       "   'train',\n",
       "   'model',\n",
       "   'use',\n",
       "   'linearregress',\n",
       "   'pickl',\n",
       "   'save',\n",
       "   'train',\n",
       "   'model',\n",
       "   'disk',\n",
       "   'request',\n",
       "   'send',\n",
       "   'request',\n",
       "   'server',\n",
       "   'json',\n",
       "   'print',\n",
       "   'result',\n",
       "   'termin']],\n",
       " 'Lemma': [['',\n",
       "   'importing',\n",
       "   'library',\n",
       "   'going',\n",
       "   'use',\n",
       "   'develop',\n",
       "   'model',\n",
       "   'numpy',\n",
       "   'panda',\n",
       "   'manipulate',\n",
       "   'matrix',\n",
       "   'data',\n",
       "   'respectively',\n",
       "   'sklearnmodelselection',\n",
       "   'splitting',\n",
       "   'data',\n",
       "   'train',\n",
       "   'test',\n",
       "   'set',\n",
       "   'sklearnlinearmodel',\n",
       "   'train',\n",
       "   'model',\n",
       "   'using',\n",
       "   'linearregression',\n",
       "   'pickle',\n",
       "   'save',\n",
       "   'trained',\n",
       "   'model',\n",
       "   'disk',\n",
       "   'request',\n",
       "   'send',\n",
       "   'request',\n",
       "   'server',\n",
       "   'json',\n",
       "   'print',\n",
       "   'result',\n",
       "   'terminal']]}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser(r,d,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('result.json', 'w') as fp:\n",
    "    json.dump(p, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NB_spam_model.pkl']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(p,'NB_spam_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
